
from collections import defaultdict
from dataclasses import dataclass
import os
import random
import time
from typing import Optional
import tqdm
from mani_skill.utils import gym_utils
from mani_skill.utils.wrappers.flatten import FlattenActionSpaceWrapper, FlattenRGBDObservationWrapper
from mani_skill.utils.wrappers.record import RecordEpisode
from mani_skill.vector.wrappers.gymnasium import ManiSkillVectorEnv
import gymnasium as gym
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
import tyro
import yaml
import mani_skill.envs
from agents.client import RemoteAgent
from agents.policies import Obs

OCTO_PORT = 22000
# for detailed tracebeck while debugging new loss
torch.autograd.set_detect_anomaly(True)

@dataclass
class Args:
    exp_name: Optional[str] = "sac_eps_decr_seed_13"
    """the name of this experiment"""    
    seed: int = 13
    """seed of the experiment"""
    torch_deterministic: bool = True
    """if toggled, `torch.backends.cudnn.deterministic=False`"""
    cuda: bool = True
    """if toggled, cuda will be enabled by default"""
    track: bool = True
    """if toggled, this experiment will be tracked with Weights and Biases"""
    wandb_project_name: str = "ManiSkill"
    """the wandb's project name"""
    wandb_entity: Optional[str] = None
    """the entity (team) of wandb's project"""
    wandb_group: str = "SAC"
    """the group of the run for wandb"""
    capture_video: bool = True
    """whether to capture videos of the agent performances (check out `videos` folder)"""
    save_trajectory: bool = False
    """whether to save trajectory data into the `videos` folder"""
    save_model: bool = True
    """whether to save model into the `runs/{run_name}` folder"""
    evaluate: bool = False
    """if toggled, only runs evaluation with the given model checkpoint and saves the evaluation trajectories"""
    checkpoint: Optional[str] = None
    """path to a pretrained checkpoint file to start evaluation/training from"""
    log_freq: int = 10_000
    """logging frequency in terms of environment steps"""

    # Environment specific arguments    
    # ========================= Environment Configs =========================
    env_id: str = "PushCube-v1"
    """the id of the environment"""
    obs_mode: str = "rgb"
    """the observation mode to use"""
    include_state: bool = False
    """whether to include the state in the observation"""
    env_vectorization: str = "gpu"
    """the type of environment vectorization to use"""
    num_envs: int = 16 # 16
    """the number of parallel environments"""
    num_eval_envs: int = 1 # 16
    """the number of parallel evaluation environments"""
    partial_reset: bool = False
    """whether to let parallel environments reset upon termination instead of truncation"""
    
    eval_partial_reset: bool = False
    """whether to let parallel evaluation environments reset upon termination instead of truncation"""
    num_steps: int = 50
    """the number of steps to run in each environment per policy rollout"""
    num_eval_steps: int = 50
    """the number of steps to run in each evaluation environment during evaluation"""
    
    control_mode: Optional[str] = "pd_ee_delta_pose" # "pd_joint_delta_pos"
    """the control mode to use for the environment"""
    sim_backend: str = "gpu"
    """the simulation backend to use"""
    # ========================= Rendering Configs =========================
    render_mode: str = "rgb_array"
    """the environment rendering mode, the rnder cam is actually used as input"""
    camera_width: Optional[int] = 256
    """the width of the camera image. If none it will use the default the environment specifies"""
    camera_height: Optional[int] = 256
    """the height of the camera image. If none it will use the default the environment specifies."""

    reward_mode: str = "normalized_dense"
    schedule_type: str = "internal_only" # "octo_epsilon_decreasing" # "internal_only" # "octo_epsilon_decreasing"
    octo_probability: float = 0.5    
    
    # ================ Training specific arguments ================
    reconfiguration_freq: Optional[int] = None
    """how often to reconfigure the environment during training"""
    eval_reconfiguration_freq: Optional[int] = 1
    """for benchmarking purposes we want to reconfigure the eval environment each reset to ensure objects are randomized in some tasks"""
    eval_freq: int = 25
    """evaluation frequency in terms of iterations"""
    save_train_video_freq: Optional[int] = None
    """frequency to save training videos in terms of iterations"""
    ckpt_save_frequency: int = 16384 # 16384 # 4096

    # Algorithm specific arguments
    total_timesteps: int = 5_000 #200_000 # 1_000_000 # 1_000_000 # 1_000_000 is around 183 GB, crazy!
    """total timesteps of the experiments"""
    buffer_size: int = 1_000  #10_000 # 100_000 #  1_000_000 # 10_000 # 1_000_000 is around 183 GB, crazy!
    """the replay memory buffer size"""
    buffer_device: str = "cuda"
    """where the replay buffer is stored. Can be 'cpu' or 'cuda' for GPU"""
    gamma: float = 0.8
    """the discount factor gamma"""
    tau: float = 0.01
    """target smoothing coefficient"""
    batch_size: int = 256 # 128 # 256 # 512
    """the batch size of sample from the replay memory"""
    learning_starts: int = 100 # 4_000
    """timestep to start learning"""
    policy_lr: float = 3e-4
    """the learning rate of the policy network optimizer"""
    q_lr: float = 3e-4
    """the learning rate of the Q network network optimizer"""
    policy_frequency: int = 1
    """the frequency of training policy (delayed)"""
    target_network_frequency: int = 1  # Denis Yarats' implementation delays this by 2.
    """the frequency of updates for the target nerworks"""
    alpha: float = 0.2
    """Entropy regularization coefficient."""
    autotune: bool = True
    """automatic tuning of the entropy coefficient"""
    training_freq: int = 64
    """training frequency (in steps)"""
    utd: float = 0.25
    """update to data ratio"""
    partial_reset: bool = False
    """whether to let parallel environments reset upon termination instead of truncation"""
    bootstrap_at_done: str = "always"
    """the bootstrap method to use when a done signal is received. Can be 'always' or 'never'"""
    
    # to be filled in runtime
    grad_steps_per_iteration: int = 0
    """the number of gradient updates per iteration"""
    steps_per_env: int = 0
    """the number of steps each parallel env takes per iteration"""
    
    
class DictArray(object):
    def __init__(self, buffer_shape, element_space, data_dict=None, device=None):
        self.buffer_shape = buffer_shape
        print(f"Buffer shape is {buffer_shape}")
        if data_dict:
            self.data = data_dict
        else:
            assert isinstance(element_space, gym.spaces.dict.Dict)
            self.data = {}
            for k, v in element_space.items():
                if isinstance(v, gym.spaces.dict.Dict):
                    self.data[k] = DictArray(buffer_shape, v, device=device)
                else:
                    dtype = (torch.float32 if v.dtype in (np.float32, np.float64) else
                            torch.uint8 if v.dtype == np.uint8 else
                            torch.int16 if v.dtype == np.int16 else
                            torch.int32 if v.dtype == np.int32 else
                            v.dtype)
                    self.data[k] = torch.zeros(buffer_shape + v.shape, dtype=dtype, device=device)

    def keys(self):
        return self.data.keys()

    def __getitem__(self, index):
        if isinstance(index, str):
            return self.data[index]
        return {
            k: v[index] for k, v in self.data.items()
        }

    def __setitem__(self, index, value):
        if isinstance(index, str):
            self.data[index] = value
        for k, v in value.items():
            self.data[k][index] = v

    @property
    def shape(self):
        return self.buffer_shape

    def reshape(self, shape):
        t = len(self.buffer_shape)
        new_dict = {}
        for k,v in self.data.items():
            if isinstance(v, DictArray):
                new_dict[k] = v.reshape(shape)
            else:
                new_dict[k] = v.reshape(shape + v.shape[t:])
        new_buffer_shape = next(iter(new_dict.values())).shape[:len(shape)]
        return DictArray(new_buffer_shape, None, data_dict=new_dict)
   

@dataclass
class ReplayBufferSample:
    obs: torch.Tensor
    next_obs: torch.Tensor
    actions: torch.Tensor
    rewards: torch.Tensor
    dones: torch.Tensor
    # octo_actions: Optional[torch.Tensor] = None
    student_actions: Optional[torch.Tensor] = None
    is_octo_action: bool = False
    
class ReplayBuffer:
    def __init__(self, env, num_envs: int, buffer_size: int, storage_device: torch.device, sample_device: torch.device):
        self.buffer_size = buffer_size
        self.pos = 0
        self.full = False
        self.num_envs = num_envs
        self.storage_device = storage_device
        self.sample_device = sample_device
        print(f"Replay buffer is supposed to have total buffer_size of {buffer_size}")
        self.per_env_buffer_size = buffer_size // num_envs
        # note 128x128x3 RGB data with replay buffer size 100_000 takes up around 4.7GB of GPU memory
        # 32 parallel envs with rendering uses up around 2.2GB of GPU memory.
        print(f"(self.per_env_buffer_size, num_envs) : {self.per_env_buffer_size}  {num_envs}")
        self.obs = DictArray((self.per_env_buffer_size, num_envs), env.single_observation_space, device=storage_device)
        # TODO (stao): optimize final observation storage
        self.next_obs = DictArray((self.per_env_buffer_size, num_envs), env.single_observation_space, device=storage_device)
        self.actions = torch.zeros((self.per_env_buffer_size, num_envs) + env.single_action_space.shape, device=storage_device)
        self.logprobs = torch.zeros((self.per_env_buffer_size, num_envs), device=storage_device)
        self.rewards = torch.zeros((self.per_env_buffer_size, num_envs), device=storage_device)
        self.dones = torch.zeros((self.per_env_buffer_size, num_envs), device=storage_device)
        self.values = torch.zeros((self.per_env_buffer_size, num_envs), device=storage_device)
        self.student_actions = torch.zeros((self.per_env_buffer_size, num_envs) + env.single_action_space.shape, device=storage_device)
        # self.octo_actions = torch.zeros((self.per_env_buffer_size, num_envs) + env.single_action_space.shape, device=storage_device)
        self.is_octo_action = torch.zeros((self.per_env_buffer_size, num_envs), device=storage_device, dtype=torch.bool)

    def add(self, obs: torch.Tensor, next_obs: torch.Tensor, 
            action: torch.Tensor, reward: torch.Tensor, done: torch.Tensor,
            student_action: torch.Tensor, # octo_action: torch.Tensor,
            is_octo_action: torch.Tensor):
        if self.storage_device == torch.device("cpu"):
            obs = {k: v.cpu() for k, v in obs.items()}
            next_obs = {k: v.cpu() for k, v in next_obs.items()}
            action = action.cpu()
            reward = reward.cpu()
            done = done.cpu()
            student_action = student_action.cpu()
            # octo_action = octo_action.cpu()
            is_octo_action = is_octo_action.cpu()

        self.obs[self.pos] = obs
        self.next_obs[self.pos] = next_obs

        self.actions[self.pos] = action
        self.rewards[self.pos] = reward
        self.dones[self.pos] = done
        self.student_actions[self.pos] = student_action
        # self.octo_actions[self.pos] = octo_action
        self.is_octo_action[self.pos] = is_octo_action 

        self.pos += 1
        if self.pos == self.per_env_buffer_size:
            self.full = True
            self.pos = 0
    def sample(self, batch_size: int):
        if self.full:
            batch_inds = torch.randint(0, self.per_env_buffer_size, size=(batch_size, ))
        else:
            batch_inds = torch.randint(0, self.pos, size=(batch_size, ))
        env_inds = torch.randint(0, self.num_envs, size=(batch_size, ))
        obs_sample = self.obs[batch_inds, env_inds]
        next_obs_sample = self.next_obs[batch_inds, env_inds]
        obs_sample = {k: v.to(self.sample_device) for k, v in obs_sample.items()}
        next_obs_sample = {k: v.to(self.sample_device) for k, v in next_obs_sample.items()}
        return ReplayBufferSample(
            obs=obs_sample,
            next_obs=next_obs_sample,
            actions=self.actions[batch_inds, env_inds].to(self.sample_device),
            rewards=self.rewards[batch_inds, env_inds].to(self.sample_device),
            dones=self.dones[batch_inds, env_inds].to(self.sample_device),
            student_actions=self.student_actions[batch_inds, env_inds].to(self.sample_device),
            is_octo_action=self.is_octo_action[batch_inds, env_inds].to(self.sample_device),
        )


# ALGO LOGIC: initialize agent here:
class PlainConv(nn.Module):
    def __init__(self,
                 in_channels=3,
                 out_dim=256,
                 pool_feature_map=False,
                 last_act=True, # True for ConvBody, False for CNN
                 image_size=[256, 256]
                 ):
        super().__init__()
        # assume input image size is 256x256

        self.out_dim = out_dim
        self.cnn = nn.Sequential(
            nn.Conv2d(in_channels, 16, 3, padding=1, bias=True), nn.ReLU(inplace=True),
            nn.MaxPool2d(4, 4) if image_size[0] == 256 and image_size[1] == 256 else nn.MaxPool2d(2, 2),  # [64, 64]
            nn.Conv2d(16, 32, 3, padding=1, bias=True), nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),  # [32, 32]
            nn.Conv2d(32, 64, 3, padding=1, bias=True), nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),  # [16, 16]
            nn.Conv2d(64, 64, 3, padding=1, bias=True), nn.ReLU(inplace=True),
            nn.MaxPool2d(2, 2),  # [8, 8]
            nn.Conv2d(64, 64, 1, padding=0, bias=True), nn.ReLU(inplace=True),
        )
        # self.octo: RemoteAgent = RemoteAgent("localhost", OCTO_PORT, "octodist")
        if pool_feature_map:
            self.pool = nn.AdaptiveMaxPool2d((1, 1))
            self.fc = make_mlp(128, [out_dim], last_act=last_act)
        else:
            self.pool = None
            self.fc = make_mlp(64 * 8 * 8, [out_dim], last_act=last_act)

        self.reset_parameters()

    def reset_parameters(self):
        for name, module in self.named_modules():
            if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d)):
                if module.bias is not None:
                    nn.init.zeros_(module.bias)

    def forward(self, image):
        x = self.cnn(image)
        if self.pool is not None:
            x = self.pool(x)
        x = x.flatten(1)
        x = self.fc(x)
        return x


class EncoderObsWrapper(nn.Module):
    def __init__(self, encoder):
        super().__init__()
        self.encoder = encoder

    def forward(self, obs):
        if "rgb" in obs:
            rgb = obs['rgb'].float() / 255.0 # (B, H, W, 3*k)
        if "depth" in obs:
            depth = obs['depth'].float() # (B, H, W, 1*k)
        if "rgb" and "depth" in obs:
            img = torch.cat([rgb, depth], dim=3) # (B, H, W, C)
        elif "rgb" in obs:
            img = rgb
        elif "depth" in obs:
            img = depth
        else:
            raise ValueError(f"Observation dict must contain 'rgb' or 'depth'")
        img = img.permute(0, 3, 1, 2) # (B, C, H, W)
        return self.encoder(img)

def make_mlp(in_channels, mlp_channels, act_builder=nn.ReLU, last_act=True):
    c_in = in_channels
    module_list = []
    for idx, c_out in enumerate(mlp_channels):
        module_list.append(nn.Linear(c_in, c_out))
        if last_act or idx < len(mlp_channels) - 1:
            module_list.append(act_builder())
        c_in = c_out
    return nn.Sequential(*module_list)


class SoftQNetwork(nn.Module):
    def __init__(self, envs, encoder: EncoderObsWrapper):
        super().__init__()
        self.encoder = encoder
        action_dim = np.prod(envs.single_action_space.shape)
        if "state" in envs.single_observation_space.keys():
            self.use_state = True
            state_dim = envs.single_observation_space['state'].shape[0]
        else:
            self.use_state = False
            state_dim = 0
        self.mlp = make_mlp(encoder.encoder.out_dim+action_dim+state_dim, [512, 256, 1], last_act=False)

    def forward(self, obs, action, visual_feature=None, detach_encoder=False):
        if visual_feature is None:
            visual_feature = self.encoder(obs)
        if detach_encoder:
            visual_feature = visual_feature.detach()
        
        if self.use_state:
            x = torch.cat([visual_feature, obs["state"], action], dim=1)
        else:
            x = torch.cat([visual_feature, action], dim=1)
        return self.mlp(x)

from mani_skill.envs.sapien_env import BaseEnv
from datetime import datetime

class RenderedImageCustomWrapper(gym.ObservationWrapper):
    """
    Flattens the rgbd mode observations into a dictionary with two keys, "rgbd" and "state"

    Args:
        rgb (bool): Whether to include rgb images in the observation
        depth (bool): Whether to include depth images in the observation
        state (bool): Whether to include state data in the observation

    Note that the returned observations will have a "rgbd" or "rgb" or "depth" key depending on the rgb/depth bool flags.
    """

    def __init__(self, env) -> None:
        self.base_env: BaseEnv = env.unwrapped
        super().__init__(env)
        new_obs = self.observation(self.base_env._init_raw_obs)
        self.base_env.update_obs_space(new_obs)

    def observation(self, observation: dict):
        # ret = dict()
        if not hasattr(self.env, "_has_reset") or not self.env._has_reset:
            self.env.reset()
            self.env._has_reset = True
        observation["sensor_data"]["base_camera"]["rgb"] = self.env.render()
        return observation


#from octoplus.src.utils.utils_for_octo import reshape_obs_for_octo
# from octoplus.src.config_dataclasses.policy_scheduling import PolicySchedulingConfig

OCTO_PORT = 22000

# ================== # Logging # ==================================================== #
import logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)
# =================================================================================== #

INSTRUCTIONS = {
        "LiftPegUpright-v1": "lift the peg upright",
        "PegInsertionSide-v1": "insert the peg from the side",
        "PickCube-v1": "pick up the cube",
        "PlugCharger-v1": "plug the charger in",
        "PullCube-v1": "pull the cube towards the robot base",
        "PullCubeTool-v1": "pull the cube by using the red tool",
        "PushCube-v1": "push the cube away from the robot base",
        "PushT-v1": "align the T shape",
        "RollBall-v1": "push the ball",
        "StackCube-v1": "stack the red cube on the green cube",
        "PokeCube-v1": "push the cube by using the blue tool",
    }


def to_device(nested_dict, device):
    """
    Converts nested dict with tensors to device
    """
    for k, v in nested_dict.items():
        if isinstance(v, dict):
            to_device(v, device)
        else:
            nested_dict[k] = v.to(device=device)
                            
                            
class DictArray(object):
    """
    A class to manage a dictionary of arrays, where each array can have a different shape.
    This class is useful for handling complex data structures in rl,
    as observations or other data can be represented as dictionaries with nested
    structures.

    Here is used to store observations.

    Attributes:from torch.distributions.normal import Normal
        buffer_shape (tuple): The shape of the buffer.
        data (dict): A dictionary containing the data arrays.
    Methods:
        keys():
            Returns the keys of the data dictionary.
        __getitem__(index):
            Retrieves an item or a sub-dictionary from the data dictionary.
        __setitem__(index, value):
            Sets an item or a sub-dictionary in the data dictionary.
        shape():
            Returns the shape of the buffer.
        reshape(shape):
            Reshapes the data dictionary to a new shape.
    """

    def __init__(self, buffer_shape, element_space, data_dict=None, device=None):
        self.buffer_shape = buffer_shape
        if data_dict:
            self.data = data_dict
        else:
            assert isinstance(element_space, gym.spaces.dict.Dict)
            self.data = {}
            for k, v in element_space.items():
                if isinstance(v, gym.spaces.dict.Dict):
                    self.data[k] = DictArray(buffer_shape, v)
                else:
                    self.data[k] = torch.zeros(buffer_shape + v.shape).to(device)

    def keys(self):
        return self.data.keys()

    def __getitem__(self, index):
        if isinstance(index, str):
            return self.data[index]
        return {k: v[index] for k, v in self.data.items()}

    def __setitem__(self, index, value):
        if isinstance(index, str):
            self.data[index] = value
        for k, v in value.items():
            self.data[k][index] = v

    @property
    def shape(self):
        return self.buffer_shape

    def reshape(self, shape):
        t = len(self.buffer_shape)
        new_dict = {}
        for k, v in self.data.items():
            if isinstance(v, DictArray):
                new_dict[k] = v.reshape(shape)
            else:
                new_dict[k] = v.reshape(shape + v.shape[t:])
        new_buffer_shape = next(iter(new_dict.values())).shape[: len(shape)]
        return DictArray(new_buffer_shape, None, data_dict=new_dict)


class OctoPolicy:
    """
    Wrapper around Remote Agent, where Octo resides
    """
    def __init__(self, env_id:str, 
                 instruction:str,
                 batch_example:dict,
                 device:str="cuda"):
        self.octo: RemoteAgent = RemoteAgent("localhost", OCTO_PORT, "octodist")
        self.octo.setup(env_id=env_id, batch_example=batch_example)
        self.octo.reset(obs=Obs(rgb_side=None), instruction=instruction)

        self.env_id = env_id
        self.device = device
        
    def act(self, obs, pos:None|int=None):
        result = self.octo.act_batch(obs)
        return torch.tensor(result, device=self.device)
    
    

LOG_STD_MAX = 2
LOG_STD_MIN = -5

class Actor(nn.Module):
    def __init__(self, envs, sample_obs):
        super().__init__()
        action_dim = np.prod(envs.single_action_space.shape)
        if "state" in envs.single_observation_space.keys():
            state_dim = envs.single_observation_space['state'].shape[0]
        else:
            state_dim = 0
        # count number of channels and image size
        in_channels = 0
        if "rgb" in sample_obs:
            in_channels += sample_obs["rgb"].shape[-1]
            image_size = sample_obs["rgb"].shape[1:3]
        if "depth" in sample_obs:
            in_channels += sample_obs["depth"].shape[-1]
            image_size = sample_obs["depth"].shape[1:3]

        self.encoder = EncoderObsWrapper(
            PlainConv(in_channels=in_channels, out_dim=256, image_size=image_size) # assume image is 64x64
        )
        self.mlp = make_mlp(self.encoder.encoder.out_dim+state_dim, [512, 256], last_act=True)
        self.fc_mean = nn.Linear(256, action_dim)
        self.fc_logstd = nn.Linear(256, action_dim)
        # action rescaling
        self.action_scale = torch.FloatTensor((envs.single_action_space.high - envs.single_action_space.low) / 2.0)
        self.action_bias = torch.FloatTensor((envs.single_action_space.high + envs.single_action_space.low) / 2.0)


    def get_feature(self, obs, detach_encoder=False):
        visual_feature = self.encoder(obs)
        if detach_encoder:
            visual_feature = visual_feature.detach()
        if "state" in obs:
            x = torch.cat([visual_feature, obs['state']], dim=1)
        else:
            x = visual_feature
        return self.mlp(x), visual_feature

    def forward(self, obs, detach_encoder=False):
        x, visual_feature = self.get_feature(obs, detach_encoder)
        mean = self.fc_mean(x)
        log_std = self.fc_logstd(x)
        log_std = torch.tanh(log_std)
        log_std = LOG_STD_MIN + 0.5 * (LOG_STD_MAX - LOG_STD_MIN) * (log_std + 1)  # From SpinUp / Denis Yarats

        return mean, log_std, visual_feature

    def get_eval_action(self, obs):
        mean, log_std, _ = self(obs)
        action = torch.tanh(mean) * self.action_scale + self.action_bias
        return action

    def get_action(self, obs, detach_encoder=False):
        mean, log_std, visual_feature = self(obs, detach_encoder)
        std = log_std.exp()
        normal = torch.distributions.Normal(mean, std)
        x_t = normal.rsample()  # for reparameterization trick (mean + std * N(0,1))
        y_t = torch.tanh(x_t)
        action = y_t * self.action_scale + self.action_bias
        log_prob = normal.log_prob(x_t)
        # Enforcing Action Bound
        log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)
        log_prob = log_prob.sum(1, keepdim=True)
        mean = torch.tanh(mean) * self.action_scale + self.action_bias
        return action, log_prob, mean, visual_feature

    def to(self, device):
        self.action_scale = self.action_scale.to(device)
        self.action_bias = self.action_bias.to(device)
        return super().to(device)
    
    # def __call__(self, obs ):
    #     return self.get_action(obs)
    
class Logger:
    def __init__(self, tensorboard: SummaryWriter) -> None:
        self.writer = tensorboard

    def add_scalar(self, tag, scalar_value, step):
        self.writer.add_scalar(tag, scalar_value, step)

    def close(self):
        self.writer.close()


if __name__ == "__main__":
    print("the function main was called")
    args = Args() # tyro.cli(Args)
    
    args.grad_steps_per_iteration = int(args.training_freq * args.utd)
    args.steps_per_env = args.training_freq // args.num_envs
    if args.exp_name is None:
        current_datetime = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        args.exp_name = os.path.basename(__file__)[: -len(".py")]
        run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{current_datetime}__{int(time.time())}"
    else:
        run_name = args.exp_name
        
    # Save the configuration into a file
    config_path = f"runs/{run_name}/config.yaml"
    os.makedirs(os.path.dirname(config_path), exist_ok=True)
    with open(config_path, "w") as config_file:
        yaml.dump(vars(args), config_file)
    print(f"Configuration saved to {config_path}")

    # TRY NOT TO MODIFY: seeding
    random.seed(args.seed)
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    torch.backends.cudnn.deterministic = args.torch_deterministic

    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
    print(f"The recognized Environment is {device}")

    ####### Environment setup #######

    env_kwargs = dict(obs_mode=args.obs_mode,
                      render_mode=args.render_mode,
                        sim_backend=args.sim_backend,
                        reward_mode=args.reward_mode,
                        human_render_camera_configs=dict(),
                        enable_shadow=True,
                        #control_mode=args.control_mode, optional
                      )
    if args.control_mode is not None:
        env_kwargs["control_mode"] = args.control_mode
    
    # we use render camera instead as need hiddent objects! could instead modify their code
    if args.camera_width is not None:
        # this overrides every sensor used for observation generation
        env_kwargs["human_render_camera_configs"]["width"] = args.camera_width
    if args.camera_height is not None:
        env_kwargs["human_render_camera_configs"]["height"] = args.camera_height
    print("env config created")
    envs = gym.make(
        args.env_id, num_envs=args.num_envs if not args.evaluate else 1, 
        reconfiguration_freq=args.reconfiguration_freq, 
        **env_kwargs)
    eval_envs = gym.make(args.env_id,
                         num_envs=args.num_eval_envs, 
                         reconfiguration_freq=args.eval_reconfiguration_freq,
                         #human_render_camera_configs=dict(shader_pack="default"), 
                         **env_kwargs)
    print("Envs created")
    envs = RenderedImageCustomWrapper(envs)
    eval_envs = RenderedImageCustomWrapper(eval_envs)
    
    
    # can add my render wrapper on top
    envs = FlattenRGBDObservationWrapper(envs, rgb=True, depth=False, state=args.include_state)
    eval_envs = FlattenRGBDObservationWrapper(eval_envs, rgb=True, depth=False, state=args.include_state)

    # rgbd obs mode returns a dict of data, we flatten it so there is just a rgbd key and state key
    # my version does not use state, but left it for ruture uses(mentioned it in future reserch point 2)
    if isinstance(envs.action_space, gym.spaces.Dict):
        envs = FlattenActionSpaceWrapper(envs)
        eval_envs = FlattenActionSpaceWrapper(eval_envs)
    print("Env wrapped")
    if args.capture_video or args.save_trajectory:
        eval_output_dir = f"runs/{run_name}/videos"
        if args.evaluate:
            eval_output_dir = f"{os.path.dirname(args.checkpoint)}/test_videos"
        print(f"Saving eval trajectories/videos to {eval_output_dir}")
        if args.save_train_video_freq is not None:
            save_video_trigger = lambda x : (x // args.num_steps) % args.save_train_video_freq == 0
            envs = RecordEpisode(envs, output_dir=f"runs/{run_name}/train_videos", save_trajectory=False, save_video_trigger=save_video_trigger, max_steps_per_video=args.num_steps, video_fps=30)
        eval_envs = RecordEpisode(eval_envs, output_dir=eval_output_dir, save_trajectory=args.save_trajectory, save_video=args.capture_video, trajectory_name="trajectory", max_steps_per_video=args.num_eval_steps, video_fps=30)
    envs = ManiSkillVectorEnv(envs, args.num_envs, ignore_terminations=not args.partial_reset, record_metrics=True)
    eval_envs = ManiSkillVectorEnv(eval_envs, args.num_eval_envs, ignore_terminations=not args.eval_partial_reset, record_metrics=True)
    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"

    max_episode_steps = gym_utils.find_max_episode_steps_value(envs._env)
    logger = None
    if not args.evaluate:
        print("Running training")
        writer = SummaryWriter(f"runs/{run_name}")
        writer.add_text(
            "hyperparameters",
            "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
        )
        logger = Logger(tensorboard=writer)
    else:
        print("Running evaluation")

    envs.single_observation_space.dtype = np.float32
    rb = ReplayBuffer(
        env=envs,
        num_envs=args.num_envs,
        buffer_size=args.buffer_size,
        storage_device=torch.device(args.buffer_device),
        sample_device=device
    )
    
    INSTRUCTIONS = {
        "LiftPegUpright-v1": "lift the peg upright",
        "PegInsertionSide-v1": "insert the peg from the side",
        "PickCube-v1": "pick up the cube",
        "PlugCharger-v1": "plug the charger in",
        "PullCube-v1": "pull the cube towards the robot base",
        "PullCubeTool-v1": "pull the cube by using the red tool",
        "PushCube-v1": "push the cube away from the robot base",
        "PushT-v1": "align the T shape",
        "RollBall-v1": "push the ball",
        "StackCube-v1": "stack the red cube on the green cube",
        "PokeCube-v1": "push the cube by using the blue tool",
    }
    # OCTO
    # Note: the remote agetent implemented to send key "rgb"
    


    # TRY NOT TO MODIFY: start the game
    obs, info = envs.reset(seed=args.seed) # in Gymnasium, seed is given to reset() instead of seed()
    for k in obs.keys():
        obs[k] = obs[k].to(device)
    eval_obs, _ = eval_envs.reset(seed=args.seed)
    for k in eval_obs.keys():
        eval_obs[k] = eval_obs[k].to(device)

    # architecture is all actor, q-networks share the same vision encoder. Output of encoder is concatenates with any state data followed by separate MLPs.
    actor = Actor(envs, sample_obs=obs).to(device)
    qf1 = SoftQNetwork(envs, actor.encoder).to(device)
    qf2 = SoftQNetwork(envs, actor.encoder).to(device)
    qf1_target = SoftQNetwork(envs, actor.encoder).to(device)
    qf2_target = SoftQNetwork(envs, actor.encoder).to(device)
    # external policy
    if args.schedule_type != "internal_only":
        octo: OctoPolicy = OctoPolicy(env_id=args.env_id, 
                                                instruction=INSTRUCTIONS[args.env_id],
                                                batch_example={"rgb" : torch.zeros((args.num_envs, args.camera_width, args.camera_height, 3), dtype=torch.float32),},
                                                device=device)
    
    if args.checkpoint is not None:
        ckpt = torch.load(args.checkpoint)
        actor.load_state_dict(ckpt['actor'])
        qf1.load_state_dict(ckpt['qf1'])
        qf2.load_state_dict(ckpt['qf2'])
    qf1_target.load_state_dict(qf1.state_dict())
    qf2_target.load_state_dict(qf2.state_dict())
    q_optimizer = optim.Adam(
        list(qf1.mlp.parameters()) +
        list(qf2.mlp.parameters()) +
        list(qf1.encoder.parameters()),
        lr=args.q_lr)
    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)

    # Automatic entropy tuning
    if args.autotune:
        target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()
        log_alpha = torch.zeros(1, requires_grad=True, device=device)
        alpha = log_alpha.exp().item()
        a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)
    else:
        alpha = args.alpha

    global_step = 0
    global_update = 0
    learning_has_started = False

    global_steps_per_iteration = args.num_envs * (args.steps_per_env)
    pbar = tqdm.tqdm(range(args.total_timesteps))
    cumulative_times = defaultdict(float)

    while global_step < args.total_timesteps:
        if args.eval_freq > 0 and (global_step - args.training_freq) // args.eval_freq < global_step // args.eval_freq:
            # evaluate
            actor.eval()
            stime = time.perf_counter()
            eval_obs, _ = eval_envs.reset()
            
            # move obs to device
            for k in eval_obs.keys():
                eval_obs[k] = eval_obs[k].to(device)
                
            eval_metrics = defaultdict(list)
            num_episodes = 0
            for _ in range(args.num_eval_steps):
                with torch.no_grad():
                    # evaluation is done only on trained policy
                    eval_obs, eval_rew, eval_terminations, eval_truncations, eval_infos = eval_envs.step(actor.get_eval_action(eval_obs))
                    if "final_info" in eval_infos:
                        mask = eval_infos["_final_info"]
                        num_episodes += mask.sum()
                        for k, v in eval_infos["final_info"]["episode"].items():
                            eval_metrics[k].append(v)
            eval_metrics_mean = {}
            for k, v in eval_metrics.items():
                mean = torch.stack(v).float().mean()
                eval_metrics_mean[k] = mean
                if logger is not None:
                    logger.add_scalar(f"eval/{k}", mean, global_step)
            pbar.set_description(
                f"success_once: {eval_metrics_mean['success_once']:.2f}, "
                f"return: {eval_metrics_mean['return']:.2f}"
            )
            if logger is not None:
                eval_time = time.perf_counter() - stime
                cumulative_times["eval_time"] += eval_time
                logger.add_scalar("time/eval_time", eval_time, global_step)
            if args.evaluate:
                break
            actor.train()

            if args.save_model and global_step % args.ckpt_save_frequency == 0:
                model_path = f"runs/{run_name}/ckpt_{global_step}.pt"
                torch.save({
                    'actor': actor.state_dict(),
                    'qf1': qf1_target.state_dict(),
                    'qf2': qf2_target.state_dict(),
                    'log_alpha': log_alpha,
                }, model_path)
                print(f"model saved to {model_path}")

        # Collect samples from environemnts
        rollout_time = time.perf_counter()
        for local_step in range(args.steps_per_env):
            global_step += 1 * args.num_envs

            # ALGO LOGIC: put action logic here
            if not learning_has_started: # instead fill with octo actions at the begining
                # a better initialization could be octo actions
                if args.schedule_type == "internal_only":
                    actions, _, _, _ = actor.get_action(obs)
                    student_actions = actions
                    octo_actions = None
                    is_octo_action = torch.zeros_like(actions[:, 0], dtype=torch.bool)
                else: 
                    octo_actions = actions = octo.act(obs)
                    student_actions = None
                    is_octo_action = torch.ones_like(actions[:, 0], dtype=torch.bool)
                # actions = torch.tensor(envs.action_space.sample(), dtype=torch.float32, device=device)
            else:
                # here with certain probability we could put octo actions
                # should be schedulare instead with separate randomnes for each env
                # 1. predic what predictor to use internal/external
                # 2. do the batched predictions for each predictor
                if args.schedule_type == "internal_only":
                    actions, _, _, _ = actor.get_action(obs)
                    octo_actions = None
                    student_actions = actions
                    is_octo_action = torch.zeros_like(actions[:, 0], dtype=torch.bool)
                elif args.schedule_type == "octo_epsilon_decreasing": # here we do crudely for the whole batch
                    octo_prob = max(0.1, 1 - (global_step / args.total_timesteps)) # epsilon decay
                    if random.random() < octo_prob:
                        # octo.
                        octo_actions = actions = octo.act(obs)
                        student_actions = None
                        is_octo_action = torch.ones_like(actions[:, 0], dtype=torch.bool)
                    else:
                        actions, _, _, _ = actor.get_action(obs)     
                        student_actions = actions
                        octo_actions = None               
                        is_octo_action = torch.zeros_like(actions[:, 0], dtype=torch.bool)
                elif args.schedule_type == "octo_epsilon": # constant epsilon
                    octo_prob = args.scheduler_config.epsilon
                    if random.random() < octo_prob:
                        # octo.
                        octo_actions = actions = octo.act(obs)
                        student_actions = None
                        is_octo_action = torch.ones_like(actions[:, 0], dtype=torch.bool)
                    else:
                        actions, _, _, _ = actor.get_action(obs)
                        student_actions = actions
                        octo_actions = None
                        is_octo_action = torch.zeros_like(actions[:, 0], dtype=torch.bool)

                # Reward-based in not implemented here
                
                # actions, _, _, _ = actor.get_action(obs) # here we could put the scheduler and octo actions
                # actions = actions.detach()

            # TRY NOT TO MODIFY: execute the game and log data.
            next_obs, rewards, terminations, truncations, infos = envs.step(actions)
            
            # Change: move obs to device
            for k in next_obs.keys():
                next_obs[k] = next_obs[k].to(device)
                
            real_next_obs = {k:v.clone() for k, v in next_obs.items()}
            if args.bootstrap_at_done == 'never':
                need_final_obs = torch.ones_like(terminations, dtype=torch.bool)
                stop_bootstrap = truncations | terminations # always stop bootstrap when episode ends
            else:
                if args.bootstrap_at_done == 'always':
                    need_final_obs = truncations | terminations # always need final obs when episode ends
                    stop_bootstrap = torch.zeros_like(terminations, dtype=torch.bool) # never stop bootstrap
                else: # bootstrap at truncated
                    need_final_obs = truncations & (~terminations) # only need final obs when truncated and not terminated
                    stop_bootstrap = terminations # only stop bootstrap when terminated, don't stop when truncated
            if "final_info" in infos:
                final_info = infos["final_info"]
                done_mask = infos["_final_info"]
                for k in real_next_obs.keys():
                    real_next_obs[k][need_final_obs] = infos["final_observation"][k][need_final_obs].clone()
                for k, v in final_info["episode"].items():
                    logger.add_scalar(f"train/{k}", v[done_mask].float().mean(), global_step)

            rb.add(obs, real_next_obs, actions, rewards, stop_bootstrap, student_actions, is_octo_action)

            # TRY NOT TO MODIFY: CRUCIAL step easy to overlook
            obs = next_obs
        rollout_time = time.perf_counter() - rollout_time
        cumulative_times["rollout_time"] += rollout_time
        pbar.update(args.num_envs * args.steps_per_env)

        # ALGO LOGIC: training.
        if global_step < args.learning_starts:
            continue

        update_time = time.perf_counter()
        learning_has_started = True
        for local_update in range(args.grad_steps_per_iteration):
            global_update += 1
            data = rb.sample(args.batch_size)
            # mask = ~data.is_octo_action
            # if mask.any() > 0:
            #     with torch.no_grad():
            #         # Filter next_obs too!
            #         filtered_next_obs = {k: v[mask] for k, v in data.next_obs.items()}                    
            #         next_state_actions, next_state_log_pi, _, visual_feature = actor.get_action(filtered_next_obs)
            #         # Compute the target Q-value using the target networks and the next state
            #         # Note: use filtered obs in Q-target too! octo act don't have grad which causese trouble
            #         qf1_next_target = qf1_target(filtered_next_obs, next_state_actions, visual_feature)
            #         qf2_next_target = qf2_target(filtered_next_obs, next_state_actions, visual_feature)                   
            #         # Take the minimum of the two target Q-values for stability (Double Q-learning)
            #         min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi
            #         # Compute the Bellman backup for the target Q-value
            #         # Q_target = reward + discount_factor * (1 - done) * min_next_Q
            #         next_q_value = data.rewards[mask].flatten() + (1 - data.dones[mask].flatten()) * args.gamma * min_qf_next_target.view(-1)


            #     # Compute the current Q-values for the sampled batch
            #     # Q function definition is Q(s, a) = E[r + gamma * Q(s', a')]
            #     visual_feature = actor.encoder(filtered_next_obs)  # Encode the observations
            
            #     # this can be calculated on octo actions?
            #     qf1_a_values = qf1(filtered_next_obs, data.actions[mask], visual_feature).view(-1)  # Q1(s, a)
            #     qf2_a_values = qf2(filtered_next_obs, data.actions[mask], visual_feature).view(-1)  # Q2(s, a)

            #     # Compute the loss for each Q-function as the mean squared error (MSE) between current Q-values and target Q-values
            #     qf1_loss = F.mse_loss(qf1_a_values, next_q_value)  # Loss for Q1
            #     qf2_loss = F.mse_loss(qf2_a_values, next_q_value)  # Loss for Q2
            #     student_q_loss = qf1_loss + qf2_loss
            #     print(f"qf1_loss: {qf1_loss.item()}, qf2_loss: {qf2_loss.item()}")
            # else:
            #     student_q_loss = torch.tensor(0.0, device=device)
            
            # if data.is_octo_action.any():
            #     mae_loss = F.l1_loss(data.student_actions[~mask], data.actions[~mask]) # l1 loss when teacher actions were taken
            #     octo_q_loss = mae_loss
            # else:
            #     octo_q_loss = torch.tensor(0.0, device=device)
            # # Combine the losses for both Q-functions
            # qf_loss = student_q_loss # + octo_q_loss
            # print(f"qf_loss: {qf_loss.item()} student_q_loss: {student_q_loss.item()} octo_q_loss: {octo_q_loss.item()}")            

            # # Optimize the Q-functions
            # q_optimizer.zero_grad()  # Zero out the gradients
            # qf_loss.backward()  # Backpropagate the loss
            # q_optimizer.step()  # Update the Q-function parameters
            
            with torch.no_grad():
                # Compute the target Q-value using the target networks and the next state
                next_state_actions, next_state_log_pi, _, visual_feature = actor.get_action(data.next_obs)
                qf1_next_target = qf1_target(data.next_obs, next_state_actions, visual_feature)
                qf2_next_target = qf2_target(data.next_obs, next_state_actions, visual_feature)
                
                # Take the minimum of the two target Q-values for stability (Double Q-learning)
                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi
                
                # Compute the Bellman backup for the target Q-value
                # Q_target = reward + discount_factor * (1 - done) * min_next_Q
                next_q_value = data.rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * min_qf_next_target.view(-1)
                # Note: data.dones is "stop_bootstrap", which is computed earlier based on args.bootstrap_at_done

            # Compute the current Q-values for the sampled batch
            visual_feature = actor.encoder(data.obs)  # Encode the observations
            qf1_a_values = qf1(data.obs, data.actions, visual_feature).view(-1)  # Q1(s, a)
            qf2_a_values = qf2(data.obs, data.actions, visual_feature).view(-1)  # Q2(s, a)

            # Compute the loss for each Q-function as the mean squared error (MSE) between current Q-values and target Q-values
            qf1_loss = F.mse_loss(qf1_a_values, next_q_value)  # Loss for Q1
            qf2_loss = F.mse_loss(qf2_a_values, next_q_value)  # Loss for Q2

            # Combine the losses for both Q-functions
            qf_loss = qf1_loss + qf2_loss

            # Optimize the Q-functions
            q_optimizer.zero_grad()  # Zero out the gradients
            qf_loss.backward()  # Backpropagate the loss
            q_optimizer.step()  # Update the Q-function parameters


            # Update the policy network (actor)
            # This step is performed less frequently (delayed update) based on policy_frequency
            if global_update % args.policy_frequency == 0:
                # Get the policy's action and log probability for the current observations
                pi, log_pi, _, visual_feature = actor.get_action(data.obs)
                
                # Compute Q-values for the policy's action using both Q-networks
                qf1_pi = qf1(data.obs, pi, visual_feature, detach_encoder=True)
                qf2_pi = qf2(data.obs, pi, visual_feature, detach_encoder=True)
                
                # Take the minimum Q-value for stability (Double Q-learning)
                min_qf_pi = torch.min(qf1_pi, qf2_pi).view(-1)
                
                # Compute the actor loss: encourage actions with high Q-values and high entropy
                actor_loss = ((alpha * log_pi) - min_qf_pi).mean()

                # Optimize the actor network
                actor_optimizer.zero_grad()
                actor_loss.backward()
                actor_optimizer.step()

                # Automatic entropy tuning (if enabled)
                if args.autotune:
                    with torch.no_grad():
                        # Recompute log probability for the current observations
                        _, log_pi, _, _ = actor.get_action(data.obs)
                    
                    # Compute the loss for the entropy coefficient (alpha)
                    alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()

                    # Optimize the entropy coefficient
                    a_optimizer.zero_grad()
                    alpha_loss.backward()
                    a_optimizer.step()
                    
                    # Update the alpha value
                    alpha = log_alpha.exp().item()

            # update the target networks
            if global_update % args.target_network_frequency == 0:
                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):
                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
                for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):
                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
        update_time = time.perf_counter() - update_time
        cumulative_times["update_time"] += update_time

        # Log training-related data
        if (global_step - args.training_freq) // args.log_freq < global_step // args.log_freq:
            logger.add_scalar("losses/qf1_values", qf1_a_values.mean().item(), global_step)
            logger.add_scalar("losses/qf2_values", qf2_a_values.mean().item(), global_step)
            logger.add_scalar("losses/qf1_loss", qf1_loss.item(), global_step)
            logger.add_scalar("losses/qf2_loss", qf2_loss.item(), global_step)
            logger.add_scalar("losses/qf_loss", qf_loss.item() / 2.0, global_step)
            logger.add_scalar("losses/actor_loss", actor_loss.item(), global_step)
            logger.add_scalar("losses/alpha", alpha, global_step)
            logger.add_scalar("time/update_time", update_time, global_step)
            logger.add_scalar("time/rollout_time", rollout_time, global_step)
            logger.add_scalar("time/rollout_fps", global_steps_per_iteration / rollout_time, global_step)
            for k, v in cumulative_times.items():
                logger.add_scalar(f"time/total_{k}", v, global_step)
            logger.add_scalar("time/total_rollout+update_time", cumulative_times["rollout_time"] + cumulative_times["update_time"], global_step)
            if args.autotune:
                logger.add_scalar("losses/alpha_loss", alpha_loss.item(), global_step)

    if not args.evaluate and args.save_model:
        model_path = f"runs/{run_name}/final_ckpt.pt"
        torch.save({
            'actor': actor.state_dict(),
            'qf1': qf1_target.state_dict(),
            'qf2': qf2_target.state_dict(),
            'log_alpha': log_alpha,
        }, model_path)
        print(f"model saved to {model_path}")
        writer.close()
    print("Hurray training has finished!")
    # if envs is not None:
    #     envs.close()
    # if eval_envs is not None:
    #     eval_envs.close()
